{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP with Bert for Sentiment Analysis","provenance":[{"file_id":"1NvffaS8wAmCPEjU6R_tF3b8FxkdVhFR5","timestamp":1619844670847},{"file_id":"1Cv-IBOePMfD3KYiGjk5GQgdZUMWFjISi","timestamp":1585908245549},{"file_id":"14pRKq1c59pgLc1O8J5bfnKiC8AR4u9Ce","timestamp":1585306142248},{"file_id":"1uMHtkpmp1dLVd_IevvJEXNn3nR1spMGl","timestamp":1569620497190},{"file_id":"12OM3ntGfd38dUqLJ-Nk82RwKTz3POWGa","timestamp":1558700851251}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VW7hNtscKMmO"},"source":["## Sentiment vs Semantic Analysis"]},{"cell_type":"markdown","metadata":{"id":"wXiflV36KSTU"},"source":["Sentiment analysis has inherent flaws. First is what it cannot tell you because it only considers a small amount of the available data.\n","Semantic analysis allows you to cluster different data elements based on similarity, rather than preset classifications such as positive, negative and neutral.\n","\n","Sentiment Analysis deals with categorizing and grouping of opionated text into majorly three to four groups (called classes in technical terms) viz., positive denoting the state of happiness, content and satisfaction; negative referring to the state of anger, grief and discontent; neutral wherein speaker (or writer) shares abosuletly no opinion and conflict where the speaker puts forward opinions in both forms (positive and negative).\n","\n","'''\n","On the other hand, semantic analysis deals with the understanding of data under various logical clusters/meanings rather than preset categories of positive or \n","negative (or neutral or conflict). It comprises of extracting relevant meanings from the given piece of information .\n","\n","A simple example that will help better to understand the difference between sentiment and semantic analysis:\n","\n","Text: “The burger and oreo crunch at Mc’D is simply awesome and smooth on pockets! I would recommed it anyday ahead of Burger King.”\n","\n","Sentiment Analysis would simply say, ‘Hey look this review is positive. ’\n","\n","Semantic Analysis on the other hand would bring up many different information on the plate: ‘The review is about food.’, ‘The review is about the quality as \n","well as price.’, ‘Two products: burger and oreo crunch have been mentioned.’ and so on.\n","\n","With more fine grained analysis and taking the help of semantic understanding we can say the review is positive for burger and for oreo crunch wrt. to quality and price.\n","\n","So in nutshell, sentiment analysis is the study of opinionated text while semantic analysis refers to discovering of meaning of structured and relevant information/clusters/groups from the data. \n","Semantic analysis is a catalyst to sentiment analysis but they both are completely different fields of study.\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"OO115fPjKl3H"},"source":["## BERT = Bidirectional Encoder Representations from Transformers\n","### Most powerful NLP Algorithm by Google\n","\n","#### https://sdsclub.com/bert-google-nlp-algorithm/"]},{"cell_type":"markdown","metadata":{"id":"ynPRbJviL-39"},"source":["# NLP with Bert for Sentiment Analysis\n","\n","Using a model called Ktrain"]},{"cell_type":"markdown","metadata":{"id":"Z82c-Fcay0a3"},"source":["### Importing the libraries"]},{"cell_type":"code","metadata":{"id":"cq0lSGd0ZyDK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630162906589,"user_tz":-300,"elapsed":37478,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"fb4287f9-020c-4173-ee5b-21716fefaa4b"},"source":["#import ktrain model\n","#pip3 is for python 3\n","!pip3 install ktrain"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting ktrain\n","  Downloading ktrain-0.27.2.tar.gz (25.3 MB)\n","\u001b[K     |████████████████████████████████| 25.3 MB 95 kB/s \n","\u001b[?25hCollecting scikit-learn==0.23.2\n","  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 21.5 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.5)\n","Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.0)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ktrain) (5.5.0)\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 33.2 MB/s \n","\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n","Collecting cchardet\n","  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n","\u001b[K     |████████████████████████████████| 263 kB 41.7 MB/s \n","\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n","Collecting syntok\n","  Downloading syntok-1.3.1.tar.gz (23 kB)\n","Collecting seqeval==0.0.19\n","  Downloading seqeval-0.0.19.tar.gz (30 kB)\n","Collecting transformers<=4.3.3,>=4.0.0\n","  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 32.1 MB/s \n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 36.3 MB/s \n","\u001b[?25hCollecting keras_bert>=0.86.0\n","  Downloading keras-bert-0.88.0.tar.gz (26 kB)\n","Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.6.2)\n","Collecting whoosh\n","  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n","\u001b[K     |████████████████████████████████| 468 kB 44.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.19.5)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.4.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.19->ktrain) (2.6.0)\n","Collecting keras-transformer>=0.39.0\n","  Downloading keras-transformer-0.39.0.tar.gz (11 kB)\n","Collecting keras-pos-embd>=0.12.0\n","  Downloading keras-pos-embd-0.12.0.tar.gz (6.0 kB)\n","Collecting keras-multi-head>=0.28.0\n","  Downloading keras-multi-head-0.28.0.tar.gz (14 kB)\n","Collecting keras-layer-normalization>=0.15.0\n","  Downloading keras-layer-normalization-0.15.0.tar.gz (4.2 kB)\n","Collecting keras-position-wise-feed-forward>=0.7.0\n","  Downloading keras-position-wise-feed-forward-0.7.0.tar.gz (4.5 kB)\n","Collecting keras-embed-sim>=0.9.0\n","  Downloading keras-embed-sim-0.9.0.tar.gz (4.1 kB)\n","Collecting keras-self-attention>=0.50.0\n","  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0.0->ktrain) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 34.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 40.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (3.0.12)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (4.6.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (4.62.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.3.3,>=4.0.0->ktrain) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<=4.3.3,>=4.0.0->ktrain) (3.5.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (2.6.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (5.0.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (4.4.2)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (4.8.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (0.8.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (57.4.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (1.0.18)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.2.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ktrain) (0.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.3.3,>=4.0.0->ktrain) (7.1.2)\n","Building wheels for collected packages: ktrain, seqeval, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, syntok\n","  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ktrain: filename=ktrain-0.27.2-py3-none-any.whl size=25283088 sha256=c7e438374ee0e4dcc158c2e9edda7ab16994c567c5557875b042bb31fc83d19d\n","  Stored in directory: /root/.cache/pip/wheels/88/be/4a/971c83a380a40f12e877f643ca1b94dc65f528f94c88dbcff7\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.19-py3-none-any.whl size=9929 sha256=4ae6447b517324a7a9462d42d2b6bc76294121069b4d6ad89b46823d3bd48a27\n","  Stored in directory: /root/.cache/pip/wheels/f5/ac/f1/4e13d7aff05c722d142b7d20a88ad63f9aab11b895411241a4\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.88.0-py3-none-any.whl size=34204 sha256=e8a8053822ed239d6efb7140bf3707ea804a305cfde79b4826019c5db803adfb\n","  Stored in directory: /root/.cache/pip/wheels/a2/90/cd/c038f2366929a3a5e3414a303b673e10235e802d871d29a835\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.39.0-py3-none-any.whl size=12842 sha256=1bdc031cd6b27320881c09e1bbc32b304e243cce9a0dea2c5952e481113ad388\n","  Stored in directory: /root/.cache/pip/wheels/bc/01/e0/5a1a14bed6726f2ed73f7917d2d2c2d4081d2c88426dea07ce\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.9.0-py3-none-any.whl size=4504 sha256=2450587d2f1c5b81cd30da1f0da5c42141411d72c9d4482c282498902c820cb8\n","  Stored in directory: /root/.cache/pip/wheels/a8/1e/d2/9bc15513dd2f8b9de3e628b3aa9d2de49e721deef6bbd1497e\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.15.0-py3-none-any.whl size=5224 sha256=66820f6f6640438bc1028c7303d12529b3bf1b0881e4725cb30cee51a89be3e3\n","  Stored in directory: /root/.cache/pip/wheels/4d/be/fe/55422f77ac11fe6ddcb471198038de8a26b5a4dd1557883c1e\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.28.0-py3-none-any.whl size=15559 sha256=c16f641461552f0297ba720028eb4d09eca5d83bdcd2966f8aa6f1738c869264\n","  Stored in directory: /root/.cache/pip/wheels/79/4a/ea/9503ab5a02201dfb8635ba2cc8f30844661623c684a5b44472\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.12.0-py3-none-any.whl size=7469 sha256=033c3344a60f60d5ef8fdab4ca02393680c22c2218bc423650c54e529b268dca\n","  Stored in directory: /root/.cache/pip/wheels/77/99/fd/dd98f4876c3ebbef7aab0dbfbd37bca41d7db37d3a28b2cb09\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.7.0-py3-none-any.whl size=5541 sha256=5f96f8321ec98d771233dbdb8d91e003ac7d93757cb6f3a7652da7949cff9835\n","  Stored in directory: /root/.cache/pip/wheels/2d/12/02/1ad455c4f181cda1a4e60c5445855853d5c2ea91f942586a04\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19414 sha256=f9ed4a4f3c6bebcd5ab768f985f39844a9eb3a78e1d93231870daf0576ff98f5\n","  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=3c504255beb501ea5eed6db3a216cd3a6c85ba52e37db94a6d405e8b49edacb2\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for syntok: filename=syntok-1.3.1-py3-none-any.whl size=20917 sha256=b4c2d7104d072e0b66db206d623bdf0536acff8c90bad113309efd306ac1979c\n","  Stored in directory: /root/.cache/pip/wheels/5e/c2/33/e5d7d8f2f8b0c391d76bf82b844c3151bf23a84d75d02b185f\n","Successfully built ktrain seqeval keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect syntok\n","Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, threadpoolctl, sacremoses, keras-transformer, whoosh, transformers, syntok, seqeval, sentencepiece, scikit-learn, langdetect, keras-bert, cchardet, ktrain\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed cchardet-2.1.7 keras-bert-0.88.0 keras-embed-sim-0.9.0 keras-layer-normalization-0.15.0 keras-multi-head-0.28.0 keras-pos-embd-0.12.0 keras-position-wise-feed-forward-0.7.0 keras-self-attention-0.50.0 keras-transformer-0.39.0 ktrain-0.27.2 langdetect-1.0.9 sacremoses-0.0.45 scikit-learn-0.23.2 sentencepiece-0.1.96 seqeval-0.0.19 syntok-1.3.1 threadpoolctl-2.2.0 tokenizers-0.10.3 transformers-4.3.3 whoosh-2.7.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ynShOu8nNtFt","executionInfo":{"status":"ok","timestamp":1630162917391,"user_tz":-300,"elapsed":10812,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["import os.path #to be used to load imdb dataset into notebook\n","import numpy as np\n","import tensorflow as tf\n","import ktrain\n","from ktrain import text"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JEjlM2EazOf0"},"source":["## Part 1: Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"ePywR8A4zaxT"},"source":["### Loading the IMDB dataset"]},{"cell_type":"code","metadata":{"id":"6kCTV_hjOKmE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630162963982,"user_tz":-300,"elapsed":46603,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"97ab2694-ef97-4ffa-f747-07943e3e567a"},"source":["#we need to load our data from the stanford website. We can't load from the zip file we downloaded\n","dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\",\n","                                  origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n","                                  extract=True)\n","IMDB_DATADIR = os.path.join(os.path.dirname(dataset), 'aclImdb')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","84131840/84125825 [==============================] - 2s 0us/step\n","84140032/84125825 [==============================] - 2s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"avEK_WWyVlgy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630162963984,"user_tz":-300,"elapsed":14,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"afa74501-2347-44df-a5d8-b849aeeb50a5"},"source":["print(os.path.dirname(dataset))\n","print(IMDB_DATADIR)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/root/.keras/datasets\n","/root/.keras/datasets/aclImdb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r8xLaiKHhW-z"},"source":["### Creating the training and test sets"]},{"cell_type":"code","metadata":{"id":"_v1UUUg1hY4a","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1630163167185,"user_tz":-300,"elapsed":203211,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"b61bd0e8-0205-469b-e74e-1fc595da7673"},"source":["#using a keras function called TEXT FROM FOLDER\n","(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder(datadir=IMDB_DATADIR,\n","                                                                       classes=['pos','neg'],\n","                                                                       maxlen=500,    #max words we are taking from the review\n","                                                                       train_test_names=['train','test'],\n","                                                                       preprocess_mode='bert')#using bert here"],"execution_count":6,"outputs":[{"output_type":"stream","text":["detected encoding: utf-8\n","downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n","[██████████████████████████████████████████████████]\n","extracting pretrained BERT model...\n","done.\n","\n","cleanup downloaded zip...\n","done.\n","\n","preprocessing train...\n","language: en\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["done."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["Is Multi-Label? False\n","preprocessing test...\n","language: en\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["done."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Bni9P0B0hpu8"},"source":["## Part 2: Building the BERT model"]},{"cell_type":"code","metadata":{"id":"ec0gNVP9s--C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630163179088,"user_tz":-300,"elapsed":11911,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"4a7a3bf5-f820-4782-b35a-d55ca4b6dd32"},"source":["model = text.text_classifier(name='bert',\n","                             train_data=(x_train, y_train),\n","                             preproc=preproc)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Is Multi-Label? False\n","maxlen is 500\n","done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Aa8pQ8-bhx0Z"},"source":["## Part 3: Training the BERT model"]},{"cell_type":"code","metadata":{"id":"voHx9EOPvSb1","executionInfo":{"status":"ok","timestamp":1630163180534,"user_tz":-300,"elapsed":1451,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["learner = ktrain.get_learner(model=model,\n","                             train_data=(x_train, y_train),\n","                             val_data=(x_test, y_test),\n","                             batch_size=6)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVfEfoRuxWSe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630171507745,"user_tz":-300,"elapsed":843783,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"b9258b83-0833-4d1e-8b21-0faf2a503ec5"},"source":["learner.fit_onecycle(lr=2e-5,\n","                     epochs=1)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n","\n","begin training using onecycle policy with max lr of 2e-05...\n","4167/4167 [==============================] - 8313s 2s/step - loss: 0.2553 - accuracy: 0.8956 - val_loss: 0.1621 - val_accuracy: 0.9392\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f4002818910>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"S_zi8fjaRuCI","executionInfo":{"status":"ok","timestamp":1630171507750,"user_tz":-300,"elapsed":7,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":[""],"execution_count":9,"outputs":[]}]}