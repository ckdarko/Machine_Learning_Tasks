{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"convolutional_neural_network.ipynb","provenance":[{"file_id":"1Y-a4g98w93GHswXLRLoiogvMYNPgzPE9","timestamp":1620044485815}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3DR-eO17geWu"},"source":["# Convolutional Neural Network"]},{"cell_type":"code","metadata":{"id":"imIniBKBSqDL","executionInfo":{"status":"ok","timestamp":1630155187937,"user_tz":-300,"elapsed":8,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["# Image Input-->[feature detector + ReLU][Convolution]-->Convolutional layer-->[Max Pooling]-->Pooling layer-->Flattening->ANN\n","# In CNN, not only the weights are adjusted in backpropagation but the feature detectors are also adjusted\n","\n","#CNN to be implemented in Jupyter Notebook because of the size of the dataset"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMefrVPCg-60"},"source":["### Importing the libraries"]},{"cell_type":"code","metadata":{"id":"sCV30xyVhFbE","executionInfo":{"status":"ok","timestamp":1630155194775,"user_tz":-300,"elapsed":4110,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}}},"source":["#we are dealing with only images so the preprocessing stage is kinda different from the classic way\n","import tensorflow as tf\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIleuCAjoFD8","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1630155197650,"user_tz":-300,"elapsed":338,"user":{"displayName":"CALEB KWASI DARKO","photoUrl":"","userId":"02161131878812025596"}},"outputId":"d3a32960-458c-4ced-b6ab-d9cb920a4332"},"source":["tf.__version__"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.6.0'"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"oxQxCBWyoGPE"},"source":["## Part 1 - Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"MvE-heJNo3GG"},"source":["### Preprocessing the Training set"]},{"cell_type":"code","metadata":{"id":"0koUcJMJpEBD"},"source":["#we have to apply transformations on all images of the training set only.\n","#this is to avoid overfitting\n","#What are these transformations--> Zooms, Rotations, Flip, Shear, etc [Coursera]. The process is called Image Augmentation\n","\n","# https://keras.io/api/preprocessing/image/\n","\n","train_datagen = ImageDataGenerator(rescale = 1./255,  #applies feature scalling to pixels by dividing all by 255 to give a range of 0-1\n","                                   shear_range = 0.2, #taken from keras api\n","                                   zoom_range = 0.2,  #taken from keras api\n","                                   horizontal_flip = True)  #taken from keras api\n","#Above is the object taken downloaded from keras \n","#we now need to connect this to our training set images, also copied from Keras        \n","#NOT SURE OF THE LOCATION                           \n","training_set = train_datagen.flow_from_directory('dataset/training_set',#using jupyter notebook  #a path to our training set\n","                                                 target_size = (64, 64), #150, 150 made the training process very long\n","                                                 batch_size = 32, #number of images we want in each batch. 32 is default\n","                                                 class_mode = 'binary') #cat or dog, so we chose binary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrCMmGw9pHys"},"source":["### Preprocessing the Test set"]},{"cell_type":"code","metadata":{"id":"SH4WzfOhpKc3"},"source":["#test transformation isn't applied to test set\n","#only the pixels must be rescaled\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","test_set = test_datagen.flow_from_directory('dataset/test_set',#using jupyter notebook\n","                                            target_size = (64, 64),\n","                                            batch_size = 32,\n","                                            class_mode = 'binary')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"af8O4l90gk7B"},"source":["## Part 2 - Building the CNN"]},{"cell_type":"markdown","metadata":{"id":"ces1gXY2lmoX"},"source":["### Initialising the CNN"]},{"cell_type":"code","metadata":{"id":"SAUt4UMPlhLS"},"source":["cnn = tf.keras.models.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5YJj_XMl5LF"},"source":["### Step 1 - Convolution"]},{"cell_type":"code","metadata":{"id":"XPzPrMckl-hV"},"source":["cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3])) \n","#classic filter is 32 [32 in first convulational layer and 32 for the next conv layer], you can tweak it\n","#kernel size = feature detector being 3x3 matrix\n","#[64, 64, 3] this is for coloured pics. For B/W, change 3 to 1\n","#usng 64,64 because we changed our target_size to 64,64\n","#using Rectifier Activation function"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tf87FpvxmNOJ"},"source":["### Step 2 - Pooling"]},{"cell_type":"code","metadata":{"id":"ncpqPl69mOac"},"source":["#applying max pooling, also in layers module\n","#A 2x2 matrix was applied to the Feature Map to obtain the Pooled Feature Map\n","#A stride of 2 was applied\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xaTOgD8rm4mU"},"source":["### Adding a second convolutional layer"]},{"cell_type":"code","metadata":{"id":"i_-FZjn_m8gk"},"source":["#copied steps 1 and 2 and just deleted the Input Shape\n","#using Rectifier Activation function\n","cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tmiEuvTunKfk"},"source":["### Step 3 - Flattening"]},{"cell_type":"code","metadata":{"id":"6AZeOGCvnNZn"},"source":["cnn.add(tf.keras.layers.Flatten())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAoSECOm203v"},"source":["### Step 4 - Full Connection"]},{"cell_type":"code","metadata":{"id":"8GtmUlLd26Nq"},"source":["#adding a fully connected neural network\n","#we increased the number of hiddent neurons to 128 instead 6 like in the ANN\n","#using Rectifier Activation function\n","cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTldFvbX28Na"},"source":["### Step 5 - Output Layer"]},{"cell_type":"code","metadata":{"id":"1p_Zj1Mc3Ko_"},"source":["#adding a Fully Connected Output layer\n","#Just one output because its binary classification\n","#activation for output for this classifier should be SIGMOID instead of SOFTMAX\n","cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D6XkI90snSDl"},"source":["## Part 3 - Training the CNN"]},{"cell_type":"markdown","metadata":{"id":"vfrFQACEnc6i"},"source":["### Compiling the CNN"]},{"cell_type":"code","metadata":{"id":"NALksrNQpUlJ"},"source":["#still using binary classification\n","cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehS-v3MIpX2h"},"source":["### Training the CNN on the Training set and evaluating it on the Test set"]},{"cell_type":"code","metadata":{"id":"XUj1W4PJptta"},"source":["#training and testing happen at the same time\n","#still using the fit method\n","#training set indicated in preprocessing step\n","#10 wasn't enough, 15 also wasn't converging. \n","#25 was better, taking about 15-20 minutes\n","cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U3PZasO0006Z"},"source":["## Part 4 - Making a single prediction"]},{"cell_type":"code","metadata":{"id":"gsSiWEJY1BPB"},"source":["import numpy as np\n","from keras.preprocessing import image\n","#load the single image\n","#image.load_img loads image as PIL\n","test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n","#converting the PIL image to a numpy array required by the predict method\n","test_image = image.img_to_array(test_image)\n","#adding an extra dimension to image which corresponds to the batch size of images  [each batch has 32 images, abatch size = 32]\n","test_image = np.expand_dims(test_image, axis = 0)\n","#result here gives either a 0 or 1 response\n","result = cnn.predict(test_image)\n","#writing our code to intepret the results.\n","#class_indices is from keras\n","training_set.class_indices\n","if result[0][0] == 1: #indexing depends on location of data\n","       #first [0] = the batch\n","       #second [0] = the one and only element in the batch (Prediction folder) \n","       \n","       #HOW DID YOU KNOW THAT 1 IS FOR DOG??\n","\n","  prediction = 'dog'\n","else:\n","  prediction = 'cat'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9ifgN9tpqEc"},"source":["print(prediction)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMTVMqvJpda7"},"source":["#download ipnyb file"],"execution_count":null,"outputs":[]}]}